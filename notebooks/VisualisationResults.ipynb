{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will analyse the results coming from the GARG-AML scores, AutoAudit and Flowscope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    'flowscope', \n",
    "    'autoaudit', \n",
    "    'gargaml_u',\n",
    "    'gargaml_d', \n",
    "    'gargaml_tree_u',\n",
    "    'gargaml_boost_u', \n",
    "    'gargaml_tree_d',\n",
    "    'gargaml_boost_d'\n",
    "]\n",
    "\n",
    "patterns = [\n",
    "    'laundering', \n",
    "    'separate',\n",
    "    'new_mules', \n",
    "    'existing_mules'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "n_nodes_list = [100, 10000, 100000] # Number of nodes in the graph\n",
    "m_edges_list = [1, 2, 5] # Number of edges to attach from a new node to existing nodes\n",
    "p_edges_list = [0.001, 0.01] # Probability of adding an edge between two nodes\n",
    "generation_method_list = [\n",
    "    'Barabasi-Albert', \n",
    "    'Erdos-Renyi', \n",
    "    'Watts-Strogatz'\n",
    "    ] # Generation method for the graph\n",
    "n_patterns_list = [3, 5] # Number of smurfing patterns to add\n",
    "\n",
    "for n_nodes in n_nodes_list:\n",
    "        for n_patterns in n_patterns_list:\n",
    "            if n_patterns <= 0.06*n_nodes:\n",
    "                for generation_method in generation_method_list:\n",
    "                    if generation_method == 'Barabasi-Albert':\n",
    "                        p_edges = 0\n",
    "                        for m_edges in m_edges_list:\n",
    "                            string_name = 'synthetic_' + generation_method + '_'  + str(n_nodes) + '_' + str(m_edges) + '_' + str(p_edges) + '_' + str(n_patterns)\n",
    "                            datasets.append(string_name)\n",
    "                    if generation_method == 'Erdos-Renyi':\n",
    "                        m_edges = 0\n",
    "                        for p_edges in p_edges_list:\n",
    "                            string_name = 'synthetic_' + generation_method + '_'  + str(n_nodes) + '_' + str(m_edges) + '_' + str(p_edges) + '_' + str(n_patterns)\n",
    "                            datasets.append(string_name)\n",
    "                    if generation_method == 'Watts-Strogatz':\n",
    "                        for m_edges in m_edges_list:\n",
    "                            for p_edges in p_edges_list:\n",
    "                                string_name = 'synthetic_' + generation_method + '_'  + str(n_nodes) + '_' + str(m_edges) + '_' + str(p_edges) + '_' + str(n_patterns)\n",
    "                                datasets.append(string_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../results-aa/synthetic_autoaudit_combined.csv'\n",
    "df = pd.read_csv(file)\n",
    "df = df.rename(columns={'Unnamed: 0': 'pattern'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../results-0/synthetic_tree_False_3.csv'\n",
    "df = pd.read_csv(file)\n",
    "df = df.rename(columns={'Unnamed: 0': 'pattern'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(df[df['pattern']=='laundering'][datasets[0]].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gargaml_results(directed):\n",
    "    if directed:\n",
    "        file = '../results-0/results_performance_directed_supervised.txt'\n",
    "    else:\n",
    "        file = '../results-0/results_performance_undirected_supervised.txt'\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    results_gargaml_dict = {}\n",
    "    for line in lines:\n",
    "        model = line.split(':', maxsplit=1)[0].split()[0]\n",
    "        results = eval(line.split(':', maxsplit=1)[1])\n",
    "        results_gargaml_dict[model] = results\n",
    "    return results_gargaml_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flowscope = pd.read_csv('../results-0/results_flowscope.csv')\n",
    "results_autoaudit = pd.read_csv('../results-aa/synthetic_autoaudit_combined.csv')\n",
    "results_gargaml_undir = gargaml_results(False)\n",
    "results_gargaml_dir = gargaml_results(True)\n",
    "results_gargaml_tree_undir_3 = pd.read_csv('../results-0/synthetic_tree_False_3.csv')\n",
    "results_gargaml_tree_undir_5 = pd.read_csv('../results-0/synthetic_tree_False_5.csv')\n",
    "results_gargaml_tree_undir = results_gargaml_tree_undir_3.merge(\n",
    "    results_gargaml_tree_undir_5, \n",
    "    on='Unnamed: 0', \n",
    ")\n",
    "results_gargaml_tree_dir = pd.read_csv('../results-0/synthetic_tree_True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gargaml_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gargaml_undir[datasets[0]]['laundering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flowscope.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    dataset:{\n",
    "        model:{\n",
    "            pattern:{\n",
    "                'AUC-ROC': 0,\n",
    "                'AUC-PR': 0,\n",
    "            }\n",
    "            for pattern in patterns\n",
    "        }\n",
    "        for model in models\n",
    "    }\n",
    "    for dataset in datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        if model =='flowscope':\n",
    "            for pattern in patterns:\n",
    "                ROC, PR = tuple(eval(results_flowscope[results_flowscope['Unnamed: 0']==pattern][dataset].values[0]))\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "        if model == 'autoaudit':\n",
    "            for pattern in patterns:\n",
    "                ROC, PR = tuple(eval(results_autoaudit[results_autoaudit['Unnamed: 0']==pattern][dataset].values[0]))\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "        if model == 'gargaml_u':\n",
    "            for pattern in patterns:\n",
    "                try:\n",
    "                    ROC, PR = tuple(results_gargaml_undir[dataset][pattern])\n",
    "                except:\n",
    "                    ROC = PR = 0\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "        if model == 'gargaml_d':\n",
    "            for pattern in patterns:\n",
    "                try:\n",
    "                    ROC, PR = tuple(results_gargaml_dir[dataset][pattern])\n",
    "                except:\n",
    "                    ROC = PR = 0\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "            \n",
    "        if model == 'gargaml_tree_u':\n",
    "            for pattern in patterns:\n",
    "                try:\n",
    "                    dict_values = eval(results_gargaml_tree_undir[results_gargaml_tree_undir['Unnamed: 0']==pattern][dataset].values[0])['tree']\n",
    "                    ROC = dict_values['AUC_ROC']\n",
    "                    PR = dict_values['AUC_PR']\n",
    "                except:\n",
    "                    ROC = PR = 0\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "\n",
    "        if model == 'gargaml_tree_d':\n",
    "            for pattern in patterns:\n",
    "                try:\n",
    "                    dict_values = eval(results_gargaml_tree_dir[results_gargaml_tree_dir['Unnamed: 0']==pattern][dataset].values[0])['tree']\n",
    "                    ROC = dict_values['AUC_ROC']\n",
    "                    PR = dict_values['AUC_PR']\n",
    "                except:\n",
    "                    ROC = PR = 0\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "\n",
    "        if model == 'gargaml_boost_u':\n",
    "            for pattern in patterns:\n",
    "                try:\n",
    "                    dict_values = eval(results_gargaml_tree_undir[results_gargaml_tree_undir['Unnamed: 0']==pattern][dataset].values[0])['boosting']\n",
    "                    ROC = dict_values['AUC_ROC']\n",
    "                    PR = dict_values['AUC_PR']\n",
    "                except:\n",
    "                    ROC = PR = 0\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n",
    "        if model == 'gargaml_boost_d':\n",
    "            for pattern in patterns:\n",
    "                try:\n",
    "                    dict_values = eval(results_gargaml_tree_dir[results_gargaml_tree_dir['Unnamed: 0']==pattern][dataset].values[0])['boosting']\n",
    "                    ROC = dict_values['AUC_ROC']\n",
    "                    PR = dict_values['AUC_PR']\n",
    "                except:\n",
    "                    ROC = PR = 0\n",
    "                results_dict[dataset][model][pattern]['AUC-ROC'] = ROC\n",
    "                results_dict[dataset][model][pattern]['AUC-PR'] = PR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gargaml_tree_undir.head(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for dataset, method_data in results_dict.items():\n",
    "    for method, pattern_data in method_data.items():\n",
    "        for pattern, metrics in pattern_data.items():\n",
    "            data.append({\n",
    "                'dataset': dataset,\n",
    "                'method': method,\n",
    "                'pattern': pattern,\n",
    "                'performance_metric': 'AUC-ROC', \n",
    "                'performance': metrics['AUC-ROC']\n",
    "            })\n",
    "\n",
    "            data.append({\n",
    "                'dataset': dataset,\n",
    "                'method': method,\n",
    "                'pattern': pattern,\n",
    "                'performance_metric': 'AUC-PR', \n",
    "                'performance': metrics['AUC-PR']\n",
    "            })\n",
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "for i in range(2):\n",
    "    for j in range(2): \n",
    "        pattern = patterns[i+2*j]\n",
    "\n",
    "        sns.boxplot(ax=axes[i,j], x='performance_metric', y='performance', hue='method', data=df[df['pattern']==pattern], palette='tab10',showmeans=True, meanprops={'marker':'*', 'markerfacecolor':'xkcd:steel', 'markeredgecolor':'.3', 'markersize': 10}, medianprops={'color': 'black', 'linewidth':2,'label': '_median_', 'linewidth':3})\n",
    "        axes[i,j].set_title('Performance for the different methods - Pattern: '+ pattern)\n",
    "        axes[i,j].set_xlabel('Performance Metric')\n",
    "        axes[i,j].set_ylabel('Value')\n",
    "        axes[i,j].legend(title='method', bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "        axes[i,j].grid(True,which=\"both\",ls=\"--\",c='gray', alpha=0.3)\n",
    "\n",
    "        handles, labels = axes[i,j].get_legend_handles_labels()\n",
    "        axes[i,j].legend(handles=handles,\n",
    "                 labels=['FlowScope', 'AutoAudit', 'GARG-AML', 'GARG-AML+Tree'],\n",
    "                 title='Method',\n",
    "                 bbox_to_anchor=(1.0, 1),\n",
    "                 loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../results/boxplot_performance_full.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_nodes in n_nodes_list:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "    for i in range(2):\n",
    "        for j in range(2): \n",
    "            pattern = patterns[i+2*j]\n",
    "\n",
    "            sns.boxplot(ax=axes[i,j], x='performance_metric', y='performance', hue='method', data=df[(df['pattern']==pattern)&(df['dataset'].str.contains('_'+str(n_nodes)+'_'))], palette='tab10',showmeans=True, meanprops={'marker':'*', 'markerfacecolor':'xkcd:steel', 'markeredgecolor':'.3', 'markersize': 10}, medianprops={'color': 'black', 'linewidth':2,'label': '_median_', 'linewidth':3})\n",
    "            axes[i,j].set_title('Performance for the different methods - Pattern: '+ pattern)\n",
    "            axes[i,j].set_xlabel('Performance Metric')\n",
    "            axes[i,j].set_ylabel('Value')\n",
    "            axes[i,j].legend(title='method', bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "            axes[i,j].grid(True,which=\"both\",ls=\"--\",c='gray', alpha=0.3)\n",
    "\n",
    "    fig.suptitle(f'Performance Analysis for #Nodes: {n_nodes}', fontsize=16)\n",
    "    plt.tight_layout()  # Adjust layout to fit the global title\n",
    "\n",
    "    plt.savefig('../results/boxplot_performance_'+str(n_nodes)+'_full.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_order(models, pattern, dataset):\n",
    "    # Initialize a dictionary to store the results\n",
    "    results = {model: {'AUC-ROC': 0, 'AUC-PR': 0} for model in models}\n",
    "\n",
    "    for model in models:\n",
    "        results[model]['AUC-ROC'] = results_dict[dataset][model][pattern]['AUC-ROC']\n",
    "        results[model]['AUC-PR'] = results_dict[dataset][model][pattern]['AUC-PR']\n",
    "\n",
    "    sorted_models_ROC = []\n",
    "    sorted_models_PR = []\n",
    "\n",
    "    sorted_models_ROC_ = sorted(models, key=lambda model: results[model]['AUC-ROC'], reverse=True)\n",
    "    sorted_models_PR_ = sorted(models, key=lambda model: results[model]['AUC-PR'], reverse=True)\n",
    "    sorted_models_ROC.append(sorted_models_ROC_)\n",
    "    sorted_models_PR.append(sorted_models_PR_)\n",
    "\n",
    "    return sorted_models_ROC, sorted_models_PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "\n",
    "for idx, pattern in enumerate(patterns):\n",
    "    sorted_models_ROC_list = []\n",
    "    sorted_models_PR_list = []\n",
    "\n",
    "    for dataset in datasets:\n",
    "        order_ROC, order_PR = give_order(models, pattern, dataset)\n",
    "        for i in order_ROC:\n",
    "            sorted_models_ROC_list.append(i)\n",
    "        for i in order_PR:\n",
    "            sorted_models_PR_list.append(i)\n",
    "    \n",
    "    dict_order_ROC = {}\n",
    "    dict_order_PR = {}\n",
    "\n",
    "    for model in models:\n",
    "        dict_order_ROC[model] = []\n",
    "        dict_order_PR[model] = []\n",
    "        for i in sorted_models_ROC_list:\n",
    "            dict_order_ROC[model].append(i.index(model) + 1)\n",
    "        for i in sorted_models_PR_list:\n",
    "            dict_order_PR[model].append(i.index(model) + 1)\n",
    "\n",
    "    combined_data = []\n",
    "    for model in models:\n",
    "        for value in dict_order_ROC[model]:\n",
    "            combined_data.append({'Method': model, 'Rank': value, 'Metric': 'AUC-ROC'})\n",
    "        for value in dict_order_PR[model]:\n",
    "            combined_data.append({'Method': model, 'Rank': value, 'Metric': 'AUC-PR'})\n",
    "    \n",
    "    combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "    # Plot the boxplot\n",
    "    sns.boxplot(ax=axes[idx//2, idx%2], x='Method', y='Rank', hue='Metric', data=combined_df, palette='tab10', showmeans=True, medianprops={'color': 'black', 'linewidth':2,'label': '_median_', 'linewidth':3}, meanprops={'marker':'*', 'markerfacecolor':'xkcd:steel', 'markeredgecolor':'.3', 'markersize': 10})\n",
    "    axes[idx//2, idx%2].set_title('Rank of the methods - Pattern: '+ pattern)\n",
    "    axes[idx//2, idx%2].set_xlabel('Method')\n",
    "    axes[idx//2, idx%2].set_ylabel('Rank')\n",
    "    axes[idx//2, idx%2].legend(title='Metric', bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "    axes[idx//2, idx%2].grid(True,which=\"both\",ls=\"--\",c='gray', alpha=0.3)  \n",
    "\n",
    "    axes[idx//2, idx%2].set_xticklabels(['FlowScope', 'AutoAudit', 'GARG-AML', 'GARG-AML+Tree'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/boxplot_rank_full.pdf')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_nodes in n_nodes_list:\n",
    "    datasets_nodes = [dataset for dataset in datasets if dataset.split('_')[2] == str(n_nodes)]    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 6))\n",
    "\n",
    "    for idx, pattern in enumerate(patterns):\n",
    "        sorted_models_ROC_list = []\n",
    "        sorted_models_PR_list = []\n",
    "\n",
    "        for dataset in datasets_nodes:\n",
    "            order_ROC, order_PR = give_order(models, pattern, dataset)\n",
    "            for i in order_ROC:\n",
    "                sorted_models_ROC_list.append(i)\n",
    "            for i in order_PR:\n",
    "                sorted_models_PR_list.append(i)\n",
    "        \n",
    "        dict_order_ROC = {}\n",
    "        dict_order_PR = {}\n",
    "\n",
    "        for model in models:\n",
    "            dict_order_ROC[model] = []\n",
    "            dict_order_PR[model] = []\n",
    "            for i in sorted_models_ROC_list:\n",
    "                dict_order_ROC[model].append(i.index(model) + 1)\n",
    "            for i in sorted_models_PR_list:\n",
    "                dict_order_PR[model].append(i.index(model) + 1)\n",
    "\n",
    "        combined_data = []\n",
    "        for model in models:\n",
    "            for value in dict_order_ROC[model]:\n",
    "                combined_data.append({'Method': model, 'Rank': value, 'Metric': 'AUC-ROC'})\n",
    "            for value in dict_order_PR[model]:\n",
    "                combined_data.append({'Method': model, 'Rank': value, 'Metric': 'AUC-PR'})\n",
    "        \n",
    "        combined_df = pd.DataFrame(combined_data)\n",
    "\n",
    "        # Plot the boxplot\n",
    "        sns.boxplot(ax=axes[idx//2, idx%2], x='Method', y='Rank', hue='Metric', data=combined_df, palette='tab10', showmeans=True, medianprops={'color': 'black', 'linewidth':2,'label': '_median_', 'linewidth':3}, meanprops={'marker':'*', 'markerfacecolor':'xkcd:steel', 'markeredgecolor':'.3', 'markersize': 10})\n",
    "        axes[idx//2, idx%2].set_title('Rank of the methods - Pattern: '+ pattern)\n",
    "        axes[idx//2, idx%2].set_xlabel('Method')\n",
    "        axes[idx//2, idx%2].set_ylabel('Rank')\n",
    "        axes[idx//2, idx%2].legend(title='Metric', bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "        axes[idx//2, idx%2].grid(True,which=\"both\",ls=\"--\",c='gray', alpha=0.3)  \n",
    "    fig.suptitle(f'Rank Analysis for #Nodes: {n_nodes}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('boxplot_rank_'+str(n_nodes)+'_full.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical test of the ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two statistical tests are applied. First, the Friedman test is used to test if there are statistically significant differences in the mean ranks of the methods. If there is, we apply the post-hoc Nemenyi test to compare the methods two-by-two and to construct the critical distance diagrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedman Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scores of 4 classifiers over 6 datasets\n",
    "# Rows = datasets, Columns = models\n",
    "scores = np.array([\n",
    "    # Model A, Model B, Model C, Model D\n",
    "    [0.85, 0.80, 0.82, 0.88], # dataset 1\n",
    "    [0.83, 0.79, 0.84, 0.85], # dataset 2\n",
    "    [0.89, 0.88, 0.86, 0.90],\n",
    "    [0.78, 0.76, 0.80, 0.79],\n",
    "    [0.92, 0.91, 0.89, 0.94],\n",
    "    [0.84, 0.82, 0.83, 0.87]\n",
    "])\n",
    "\n",
    "# Run the Friedman test\n",
    "stat, p = friedmanchisquare(*scores.T)\n",
    "\n",
    "print(f\"Friedman test statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Significant differences found — proceeding with Critical Distance Diagram\")\n",
    "\n",
    "    # Compute ranks (1 = best, higher = worse)\n",
    "    ranks = np.argsort(np.argsort(-scores, axis=1), axis=1) + 1  # descending order\n",
    "\n",
    "    # Average ranks per model\n",
    "    avg_ranks = np.mean(ranks, axis=0)\n",
    "    print(\"Average ranks:\", avg_ranks)\n",
    "\n",
    "    nf = sp.posthoc_nemenyi_friedman(scores)\n",
    "    nf.index=['Model A', 'Model B', 'Model C', 'Model D']\n",
    "    nf.columns=['Model A', 'Model B', 'Model C', 'Model D']\n",
    "    # Format: diagonal, non-significant, p<0.001, p<0.01, p<0.05\n",
    "    cmap = ['1', '#fb6a4a',  '#08306b',  '#4292c6', '#c6dbef']\n",
    "    heatmap_args = {'cmap': cmap, 'linewidths': 0.25, 'linecolor': '0.5', 'square': True}\n",
    "    sp.sign_plot(nf, **heatmap_args)\n",
    "\n",
    "else:\n",
    "    print(\"No statistically significant differences found — skipping post-hoc analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ranks_dict = {}\n",
    "for i, model in enumerate(nf.index):\n",
    "    avg_ranks_dict[model] = avg_ranks[i]\n",
    "avg_ranks_dict\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.title('Critical difference diagram of average score ranks')\n",
    "sp.critical_difference_diagram(avg_ranks_dict, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection = df[(df['pattern']=='laundering')&(df['performance_metric']=='AUC-ROC')][['dataset', 'method', 'performance']]\n",
    "avg_rank = df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean()\n",
    "avg_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_data = {}\n",
    "for method in models:\n",
    "    dict_data[method] = []\n",
    "    for dataset in datasets:\n",
    "        dict_data[method].append(results_dict[dataset][method]['laundering']['AUC-ROC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array(list(dict_data.values())).T\n",
    "\n",
    "# Run the Friedman test\n",
    "stat, p = friedmanchisquare(*scores.T)\n",
    "\n",
    "print(f\"Friedman test statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "\n",
    "if p<0.05:\n",
    "    print(\"Significant differences found — proceeding with Critical Distance Diagram\")\n",
    "\n",
    "    # Compute ranks (1 = best, higher = worse)\n",
    "    ranks = np.argsort(np.argsort(-scores, axis=1), axis=1) + 1  # descending order\n",
    "\n",
    "    # Average ranks per model\n",
    "    avg_ranks = np.mean(ranks, axis=0)\n",
    "    print(\"Average ranks:\", avg_ranks)\n",
    "\n",
    "    nf = sp.posthoc_nemenyi_friedman(scores)\n",
    "    model_names = list(dict_data.keys())\n",
    "    nf.index=nf.columns=model_names\n",
    "    \n",
    "    # Format: diagonal, non-significant, p<0.001, p<0.01, p<0.05\n",
    "    cmap = ['1', '#fb6a4a',  '#08306b',  '#4292c6', '#c6dbef']\n",
    "    heatmap_args = {'cmap': cmap, 'linewidths': 0.25, 'linecolor': '0.5', 'square': True}\n",
    "    sp.sign_plot(nf, **heatmap_args)\n",
    "\n",
    "else:\n",
    "    print(\"No statistically significant differences found — skipping post-hoc analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the colors for the models based on their order in the boxplot\n",
    "model_colors = {\n",
    "    'flowscope': 'tab:blue',\n",
    "    'autoaudit': 'tab:orange',\n",
    "    'gargaml_u': 'tab:green',\n",
    "    #'gargaml_d': 'tab:red', \n",
    "    'gargaml_tree_u': 'tab:red',#'tab:purple',\n",
    "    'gargaml_boost_u': 'tab:brown', \n",
    "    'gargaml_tree_d': 'tab:pink',\n",
    "    'gargaml_boost_d': 'tab:gray'\n",
    "}\n",
    "\n",
    "pretty_model_names = {\n",
    "'flowscope': 'FlowScope',\n",
    "   'autoaudit': 'AutoAudit',\n",
    "   'gargaml_u': 'GARG-AML',\n",
    "   'gargaml_d': 'GARG-AML Dir.', \n",
    "   'gargaml_tree_u': 'GARG-AML + Tree',\n",
    "   'gargaml_boost_u': 'GARG-AML Boost Undir.',\n",
    "   'gargaml_tree_d': 'GARG-AML Tree Dir.',\n",
    "   'gargaml_boost_d': 'GARG-AML Boost Dir.'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "avg_ranks_dict = dict(df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean())\n",
    "\n",
    "# --- IMPORTANT: Create a new dictionary with PRETTY labels ---\n",
    "avg_ranks_pretty = {pretty_model_names[model]: rank for model, rank in avg_ranks_dict.items()}\n",
    "nf.rename(index=pretty_model_names, columns=pretty_model_names, inplace=True)\n",
    "\n",
    "# Also adapt the color palette using pretty labels\n",
    "color_palette_pretty = {pretty_model_names[model]: model_colors[model] for model in avg_ranks_dict.keys()}\n",
    "\n",
    "# Now plot with pretty names\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.title('Critical Difference Diagram of Average Score Ranks')\n",
    "sp.critical_difference_diagram(\n",
    "    ranks=avg_ranks_pretty,\n",
    "    #ranks=avg_ranks_dict,\n",
    "    sig_matrix=nf,  # Assuming nf matches the order of avg_ranks_dict\n",
    "    label_fmt_left='{label} [{rank:.2f}]  ',\n",
    "    label_fmt_right='  [{rank:.2f}] {label}',\n",
    "    color_palette=color_palette_pretty,\n",
    "    #color_palette={model: model_colors[model] for model in avg_ranks_dict.keys()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(8, 6.5))\n",
    "\n",
    "for i_ax in range(4):\n",
    "    pattern = patterns[i_ax]\n",
    "\n",
    "    df_selection = df[(df['pattern']==pattern)&(df['performance_metric']=='AUC-ROC')][['dataset', 'method', 'performance']]\n",
    "    avg_rank = df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean()\n",
    "\n",
    "    dict_data = {}\n",
    "    for method in models:\n",
    "        dict_data[method] = []\n",
    "        for dataset in datasets:\n",
    "            dict_data[method].append(results_dict[dataset][method][pattern]['AUC-ROC'])        \n",
    "\n",
    "    scores = np.array(list(dict_data.values())).T\n",
    "\n",
    "    # Run the Friedman test\n",
    "    stat, p = friedmanchisquare(*scores.T)\n",
    "\n",
    "    print(f\"Friedman test statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "\n",
    "    if p<0.05:\n",
    "        print(\"Significant differences found — proceeding with Critical Distance Diagram\")\n",
    "\n",
    "        # Compute ranks (1 = best, higher = worse)\n",
    "        ranks = np.argsort(np.argsort(-scores, axis=1), axis=1) + 1  # descending order\n",
    "\n",
    "        # Average ranks per model\n",
    "        avg_ranks = np.mean(ranks, axis=0)\n",
    "        print(\"Average ranks:\", avg_ranks)\n",
    "\n",
    "        nf = sp.posthoc_nemenyi_friedman(scores)\n",
    "        model_names = list(dict_data.keys())\n",
    "        nf.index=nf.columns=model_names\n",
    "\n",
    "        avg_ranks_dict = dict(df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean())\n",
    "\n",
    "        # --- IMPORTANT: Create a new dictionary with PRETTY labels ---\n",
    "        avg_ranks_pretty = {pretty_model_names[model]: rank for model, rank in avg_ranks_dict.items()}\n",
    "        nf.rename(index=pretty_model_names, columns=pretty_model_names, inplace=True)\n",
    "\n",
    "        # Also adapt the color palette using pretty labels\n",
    "        color_palette_pretty = {pretty_model_names[model]: model_colors[model] for model in avg_ranks_dict.keys()}\n",
    "        \n",
    "        axes[i_ax].set_title('Pattern: '+pattern)\n",
    "        sp.critical_difference_diagram(\n",
    "            ranks=avg_ranks_pretty,\n",
    "            # ranks=avg_ranks_dict,\n",
    "            sig_matrix=nf,\n",
    "            label_fmt_left='{label} ({rank:.2f})  ',\n",
    "            label_fmt_right='  ({rank:.2f}) {label}',\n",
    "            label_props={'fontweight': 'bold'},\n",
    "            color_palette= color_palette_pretty, \n",
    "            # color_palette={model: model_colors[model] for model in avg_ranks_dict.keys()},\n",
    "            ax=axes[i_ax]\n",
    "            )\n",
    "        \n",
    "        # Get correct handles and labels\n",
    "        handles, labels = axes[i_ax].get_legend_handles_labels()\n",
    "\n",
    "    else:\n",
    "        print(\"No statistically significant differences found — skipping post-hoc analysis.\")\n",
    "\n",
    "plt.suptitle('Critical difference diagram for average rank according to the AUC-ROC')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/CD_ROC_full.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(8, 6.5))\n",
    "\n",
    "for i_ax in range(4):\n",
    "    pattern = patterns[i_ax]\n",
    "\n",
    "    df_selection = df[(df['pattern']==pattern)&(df['performance_metric']=='AUC-PR')][['dataset', 'method', 'performance']]\n",
    "    avg_rank = df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean()\n",
    "\n",
    "    dict_data = {}\n",
    "    for method in models:\n",
    "        dict_data[method] = []\n",
    "        for dataset in datasets:\n",
    "            dict_data[method].append(results_dict[dataset][method][pattern]['AUC-PR'])        \n",
    "\n",
    "    scores = np.array(list(dict_data.values())).T\n",
    "\n",
    "    # Run the Friedman test\n",
    "    stat, p = friedmanchisquare(*scores.T)\n",
    "\n",
    "    print(f\"Friedman test statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "\n",
    "    if p<0.05:\n",
    "        print(\"Significant differences found — proceeding with Critical Distance Diagram\")\n",
    "\n",
    "        # Compute ranks (1 = best, higher = worse)\n",
    "        ranks = np.argsort(np.argsort(-scores, axis=1), axis=1) + 1  # descending order\n",
    "\n",
    "        # Average ranks per model\n",
    "        avg_ranks = np.mean(ranks, axis=0)\n",
    "        print(\"Average ranks:\", avg_ranks)\n",
    "\n",
    "        nf = sp.posthoc_nemenyi_friedman(scores)\n",
    "        model_names = list(dict_data.keys())\n",
    "        nf.index=nf.columns=model_names\n",
    "\n",
    "        avg_ranks_dict = dict(df_selection.groupby('dataset').performance.rank(ascending=False).groupby(df_selection.method).mean())\n",
    "\n",
    "        # --- IMPORTANT: Create a new dictionary with PRETTY labels ---\n",
    "        avg_ranks_pretty = {pretty_model_names[model]: rank for model, rank in avg_ranks_dict.items()}\n",
    "        nf.rename(index=pretty_model_names, columns=pretty_model_names, inplace=True)\n",
    "\n",
    "        # Also adapt the color palette using pretty labels\n",
    "        color_palette_pretty = {pretty_model_names[model]: model_colors[model] for model in avg_ranks_dict.keys()}\n",
    "\n",
    "        axes[i_ax].set_title('Pattern: '+pattern)\n",
    "        sp.critical_difference_diagram(\n",
    "            ranks=avg_ranks_pretty,\n",
    "            # ranks=avg_ranks_dict,\n",
    "            sig_matrix=nf,\n",
    "            label_fmt_left='{label} ({rank:.2f})  ',\n",
    "            label_fmt_right='  ({rank:.2f}) {label}',\n",
    "            label_props={'fontweight': 'bold'},\n",
    "            color_palette= color_palette_pretty, \n",
    "            # color_palette={model: model_colors[model] for model in avg_ranks_dict.keys()},\n",
    "            ax=axes[i_ax]\n",
    "            )\n",
    "    else:\n",
    "        print(\"No statistically significant differences found — skipping post-hoc analysis.\")\n",
    "\n",
    "plt.suptitle('Critical difference diagram for average rank according to the AUC-PR')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/CD_PR_full.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the notebook, we analyse the results for the IBM data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARG-AML Undirected\n",
    "file = '../results/results_performance_IBM_undirected.txt'\n",
    "with open(file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "model_list = []\n",
    "dataset_list = []\n",
    "pattern_list = []\n",
    "cutoff_list = []\n",
    "AUCROC_list = []\n",
    "AUCPR_list = []\n",
    "\n",
    "for line in lines:\n",
    "    long_split = line.strip().split('_')\n",
    "    model = 'GARG-AML Undirected'\n",
    "    dataset = long_split[0]\n",
    "    pattern = long_split[1]\n",
    "    cutoff_results = long_split[2].split(' ', maxsplit=3)\n",
    "    cutoff = cutoff_results[0]\n",
    "    try:\n",
    "        results  = eval(cutoff_results[-1])\n",
    "    except:\n",
    "        results = [0, 0]\n",
    "    AUCROC = results[0]\n",
    "    AUCPR = results[1]\n",
    "    model_list.append(model)\n",
    "    dataset_list.append(dataset)\n",
    "    pattern_list.append(pattern)\n",
    "    cutoff_list.append(cutoff)\n",
    "    AUCROC_list.append(AUCROC)\n",
    "    AUCPR_list.append(AUCPR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GARG-AML Directed\n",
    "file = '../results/results_performance_IBM_directed.txt'\n",
    "with open(file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    long_split = line.strip().split('_')\n",
    "    model = 'GARG-AML Directed'\n",
    "    dataset = long_split[0]\n",
    "    pattern = long_split[1]\n",
    "    cutoff_results = long_split[2].split(' ', maxsplit=3)\n",
    "    cutoff = cutoff_results[0]\n",
    "    try:\n",
    "        results  = eval(cutoff_results[-1])\n",
    "    except:\n",
    "        results = [0, 0]\n",
    "    AUCROC = results[0]\n",
    "    AUCPR = results[1]\n",
    "    model_list.append(model)\n",
    "    dataset_list.append(dataset)\n",
    "    pattern_list.append(pattern)\n",
    "    cutoff_list.append(cutoff)\n",
    "    AUCROC_list.append(AUCROC)\n",
    "    AUCPR_list.append(AUCPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowscope HI-Small\n",
    "file = '../results/flowscope_performance_HI-Small.txt'\n",
    "with open(file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    long_split = line.strip().split('_')\n",
    "    model = 'FlowScope'\n",
    "    dataset = 'HI-Small'\n",
    "    pattern = long_split[1]\n",
    "    cutoff_results = long_split[3].split(' ', maxsplit=3)\n",
    "    cutoff = cutoff_results[0]\n",
    "    try:\n",
    "        results  = eval(cutoff_results[-1])\n",
    "    except:\n",
    "        results = [0, 0]\n",
    "    AUCROC = results[0]\n",
    "    AUCPR = results[1]\n",
    "    model_list.append(model)\n",
    "    dataset_list.append(dataset)\n",
    "    pattern_list.append(pattern)\n",
    "    cutoff_list.append(cutoff)\n",
    "    AUCROC_list.append(AUCROC)\n",
    "    AUCPR_list.append(AUCPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flowscope LI-Large\n",
    "file = '../results/flowscope_performance_LI-Large.txt'\n",
    "with open(file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    long_split = line.strip().split('_')\n",
    "    print(long_split)\n",
    "    model = 'FlowScope'\n",
    "    dataset = 'LI-Large'\n",
    "    pattern = long_split[1]\n",
    "    cutoff_results = long_split[3].split(' ', maxsplit=3)\n",
    "    cutoff = cutoff_results[0]\n",
    "    try:\n",
    "        results  = eval(cutoff_results[-1])\n",
    "    except:\n",
    "        results = [0, 0]\n",
    "    AUCROC = results[0]\n",
    "    AUCPR = results[1]\n",
    "    model_list.append(model)\n",
    "    dataset_list.append(dataset)\n",
    "    pattern_list.append(pattern)\n",
    "    cutoff_list.append(cutoff)\n",
    "    AUCROC_list.append(AUCROC)\n",
    "    AUCPR_list.append(AUCPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_IBM_tree = ['tree', 'boosting']\n",
    "directed_list = ['undirected', 'directed']\n",
    "IMB_data_list = ['HI-Small', 'LI-Large']\n",
    "metrics_list = ['ROC', 'PR']\n",
    "IBM_cutoffs = [0.1, 0.2, 0.3, 0.5, 0.9]\n",
    "IBM_patterns = ['Is Laundering', 'FAN-OUT', 'FAN-IN', 'GATHER-SCATTER', 'SCATTER-GATHER', 'CYCLE', 'RANDOM', 'BIPARTITE', 'STACK']\n",
    "\n",
    "for ds in IMB_data_list:\n",
    "    for d in directed_list:\n",
    "        for m in models_IBM_tree:\n",
    "            for cut_off in IBM_cutoffs:\n",
    "                for pattern in IBM_patterns:\n",
    "                    model_list.append('GARG-AML '+d.capitalize()+' '+m.capitalize())\n",
    "                    dataset_list.append(ds)\n",
    "                    pattern_list.append(pattern)\n",
    "                    cutoff_list.append(str(cut_off))\n",
    "                    for pm in metrics_list:\n",
    "                        file = '../results/'+ds+'_AUC_'+pm+'_'+m+'_'+d+'_combined.csv'\n",
    "                        df = pd.read_csv(file, index_col=0)\n",
    "                        if pm == 'ROC':\n",
    "                            AUC = df.loc[cut_off][pattern]\n",
    "                            AUCROC_list.append(AUC)\n",
    "                        else:\n",
    "                            AUC = df.loc[cut_off][pattern]\n",
    "                            AUCPR_list.append(AUC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'model': model_list,\n",
    "    'dataset': dataset_list,\n",
    "    'pattern': pattern_list,\n",
    "    'cutoff': cutoff_list,\n",
    "    'AUC-ROC': AUCROC_list,\n",
    "    'AUC-PR': AUCPR_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_plot_df = results_df[(results_df['dataset']==dataset) & (results_df['pattern'].isin(['Is Laundering', 'GATHER-SCATTER', 'SCATTER-GATHER'])) & (results_df['cutoff'].isin(['0.1', '0.5' ,'0.9']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "datasets = ['HI-Small', 'LI-Large']\n",
    "for dataset in datasets:\n",
    "    results_plot_df = results_df[(results_df['dataset']==dataset) & (results_df['pattern'].isin(['Is Laundering', 'GATHER-SCATTER', 'SCATTER-GATHER'])) & (results_df['cutoff'].isin(['0.1', '0.5' ,'0.9']))]\n",
    "\n",
    "    # Ensure consistent aesthetics\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Assume results_plot_df is already defined\n",
    "    # Extract unique patterns and cutoffs\n",
    "    patterns = results_plot_df['pattern'].unique()\n",
    "    cutoffs = results_plot_df['cutoff'].unique()\n",
    "    models = results_plot_df['model'].unique()\n",
    "    palette = sns.color_palette(\"tab10\", len(models)+1)\n",
    "    del (palette[1])\n",
    "\n",
    "    # Create a mapping from model names to colors\n",
    "    model_color_map = dict(zip(models, palette))\n",
    "\n",
    "    # Create the subplot grid\n",
    "    fig, axes = plt.subplots(len(patterns), len(cutoffs), figsize=(7 * len(cutoffs), 3 * len(patterns)), squeeze=False)\n",
    "\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        for j, cutoff in enumerate(cutoffs):\n",
    "            ax = axes[i, j]\n",
    "            subset = results_plot_df[(results_plot_df['pattern'] == pattern) & (results_plot_df['cutoff'] == cutoff)]\n",
    "            ylim_max = subset['AUC-ROC'].max()*1.1\n",
    "            ax.set_ylim(0, ylim_max)\n",
    "            # Primary bar plot for AUC-ROC\n",
    "            for idx, model in enumerate(models):\n",
    "                model_data = subset[subset['model'] == model]\n",
    "                roc = model_data['AUC-ROC'].values[0]\n",
    "                ax.bar(idx - 0.2, model_data['AUC-ROC'], width=0.4, label=model if i == 0 and j == 0 else \"\", \n",
    "                    color=model_color_map[model], edgecolor='black')\n",
    "                ax.text(idx - 0.23, roc-0.001, f\"{roc*100:.1f}%\", ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Twin axis for AUC-PR\n",
    "            ylim_max_2 = subset['AUC-PR'].max()*1.1\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylim(0, ylim_max_2)\n",
    "            for idx, model in enumerate(models):\n",
    "                model_data = subset[subset['model'] == model]\n",
    "                pr = model_data['AUC-PR'].values[0]\n",
    "                ax2.bar(idx + 0.2, model_data['AUC-PR'], width=0.4, label=None, \n",
    "                        color=model_color_map[model], hatch='//', alpha=0.7, edgecolor='black')\n",
    "                ax2.text(idx + 0.28, pr, f\"{pr*100:.3f}%\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "            ax.set_title(f\"Pattern: {pattern} | Cutoff: {cutoff}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xticklabels([])\n",
    "            #ax.set_xticks(range(len(models)))\n",
    "            #ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "            ax.set_ylabel(\"AUC-ROC\")\n",
    "            ax2.set_ylabel(\"AUC-PR\")\n",
    "\n",
    "    # Add legend only once\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=model_color_map[m]) for m in models]\n",
    "    fig.legend(handles, models, loc='upper center', ncol=len(models), title='Models - '+dataset)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "    plt.savefig('../results/'+dataset+'_AUC-ROC_AUC-PR.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "datasets = ['HI-Small', 'LI-Large']\n",
    "for dataset in datasets:\n",
    "    results_plot_df = results_df[(results_df['dataset']==dataset)]\n",
    "\n",
    "    # Ensure consistent aesthetics\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Assume results_plot_df is already defined\n",
    "    # Extract unique patterns and cutoffs\n",
    "    patterns = results_plot_df['pattern'].unique()\n",
    "    cutoffs = results_plot_df['cutoff'].unique()\n",
    "    models = results_plot_df['model'].unique()\n",
    "    palette = sns.color_palette(\"tab10\", len(models)+1)\n",
    "    del (palette[1])\n",
    "\n",
    "    # Create a mapping from model names to colors\n",
    "    model_color_map = dict(zip(models, palette))\n",
    "\n",
    "    # Create the subplot grid\n",
    "    fig, axes = plt.subplots(len(patterns), len(cutoffs), figsize=(7 * len(cutoffs), 3 * len(patterns)), squeeze=False)\n",
    "\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        for j, cutoff in enumerate(cutoffs):\n",
    "            ax = axes[i, j]\n",
    "            subset = results_plot_df[(results_plot_df['pattern'] == pattern) & (results_plot_df['cutoff'] == cutoff)]\n",
    "            ylim_max = subset['AUC-ROC'].max()*1.1\n",
    "            ax.set_ylim(0, ylim_max)\n",
    "            # Primary bar plot for AUC-ROC\n",
    "            for idx, model in enumerate(models):\n",
    "                model_data = subset[subset['model'] == model]\n",
    "                roc = model_data['AUC-ROC'].values[0]\n",
    "                ax.bar(idx - 0.2, model_data['AUC-ROC'], width=0.4, label=model if i == 0 and j == 0 else \"\", \n",
    "                    color=model_color_map[model], edgecolor='black')\n",
    "                ax.text(idx - 0.23, roc-0.001, f\"{roc*100:.1f}%\", ha='center', va='bottom', fontsize=8)\n",
    "            \n",
    "            # Twin axis for AUC-PR\n",
    "            ylim_max_2 = subset['AUC-PR'].max()*1.1\n",
    "            ax2 = ax.twinx()\n",
    "            ax2.set_ylim(0, ylim_max_2)\n",
    "            for idx, model in enumerate(models):\n",
    "                model_data = subset[subset['model'] == model]\n",
    "                pr = model_data['AUC-PR'].values[0]\n",
    "                ax2.bar(idx + 0.2, model_data['AUC-PR'], width=0.4, label=None, \n",
    "                        color=model_color_map[model], hatch='//', alpha=0.7, edgecolor='black')\n",
    "                ax2.text(idx + 0.28, pr, f\"{pr*100:.3f}%\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "            ax.set_title(f\"Pattern: {pattern} | Cutoff: {cutoff}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_xticklabels([])\n",
    "            #ax.set_xticks(range(len(models)))\n",
    "            #ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "            ax.set_ylabel(\"AUC-ROC\")\n",
    "            ax2.set_ylabel(\"AUC-PR\")\n",
    "\n",
    "    # Add legend only once\n",
    "    handles = [plt.Rectangle((0,0),1,1, color=model_color_map[m]) for m in models]\n",
    "    fig.legend(handles, models, loc='upper center', ncol=len(models), title='Models - '+dataset)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.94])\n",
    "    plt.savefig('../results/'+dataset+'_AUC-ROC_AUC-PR_full.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
